


!wget http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz
!tar xvzf ta-lib-0.4.0-src.tar.gz
%cd ta-lib/
!./configure --prefix=/usr
!make
!make install
%cd ..
!pip install TA-Lib




# Import libraries
import pandas as pd
import numpy as np
import requests
import talib
import yfinance as yf
import nltk
nltk.download('vader_lexicon')
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam
from sklearn.preprocessing import MinMaxScaler

# Define function to retrieve stock data
def get_stock_data(symbol):
    data = yf.download(symbol, start='2010-01-01')
    return data

def analyze_news_sentiment(query):
    url = ('https://newsapi.org/v2/everything?'
           'q={}&'
           'from=2021-01-01&'
           'sortBy=publishedAt&'
           'apiKey=4054ae3e71be43d89ff654b73e3c50da').format(query)
    response = requests.get(url)
    data = response.json()
    if 'article' not in data:
        print('Error: No article found in response')
        return
    articles = data['article']
    analyzer = SentimentIntensityAnalyzer()
    sentiment_scores = []
    for article in articles:
        title = article['title']
        description = article['description']
        text = title + ' ' + description
        sentiment_scores.append(analyzer.polarity_scores(text)['compound'])
    return np.mean(sentiment_scores)


# Define function to create neural network model
def create_model():
    model = Sequential()
    model.add(Dense(32, input_shape=(10,), activation='relu'))
    model.add(Dense(16, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    optimizer = Adam(lr=0.001)
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])
    return model

def make_predictions(symbol):
   def make_predictions(symbol):
    df = get_stock_data(symbol)
    close_prices = df['Close'].values
    rsi = talib.RSI(close_prices)
    rsi_rolling = pd.Series(rsi).rolling(window=10).apply(abs).values
    prediction = float(rsi_rolling.iloc[-1]) < float(rsi_rolling.iloc[-2]) and float(rsi_rolling.iloc[-2]) > float(rsi_rolling.iloc[-3])
    if prediction:
        return f"{symbol} is predicted to go up."
    else:
        return f"{symbol} is predicted to go down."

    # Calculate technical indicators
    stock_data['MA_50'] = stock_data['Close'].rolling(window=50).mean()
    stock_data['MA_200'] = stock_data['Close'].rolling(window=200).mean()
    stock_data['RSI'] = 100 - (100 / (1 + (stock_data['Close'].diff().fillna(0).rolling(window=14, center=False).mean() / 
                                          stock_data['Close'].diff().fillna(0).rolling(window=14, center=False).abs().rolling(window=14, center=False).mean())))
    stock_data['MACD'] = stock_data['Close'].ewm(span=12, adjust=False).mean() - stock_data['Close'].ewm(span=26, adjust=False).mean()
    stock_data['Signal'] = stock_data['MACD'].ewm(span=9, adjust=False).mean()
    stock_data = stock_data.dropna()
    # Calculate news sentiment
    mean_sentiment = analyze_news_sentiment(symbol)
    # Combine technical indicators and news sentiment
    features = stock_data[['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'MA_50', 'MA_200', 'RSI', 'Signal']].values
    features = np.append(features, mean_sentiment)
    features = features.reshape(1, -1)
    # Normalize features
    scaler = MinMaxScaler()
    features = scaler.fit_transform(features)
    # Load neural network model
    model.load_weights('model_weights.h5')
    # Make predictions
    prediction = model.predict(features)
    return prediction[0][0]

def find_wd_gann_cycles(symbol):
    # Define time frame
    timeframe = '1y'
    
    # Get historical data
    df = yf.download(symbol, period=timeframe, interval='1d')
    
    # Calculate the High Low Range
    df['HL Range'] = df['High'] - df['Low']
    
    # Calculate the 10-day SMA
    df['10-day SMA'] = df['Close'].rolling(window=10).mean()
    
    # Calculate the 50-day SMA
    df['50-day SMA'] = df['Close'].rolling(window=50).mean()
    
    # Define the Gann angles
    gann_angles = [1, 2, 3, 4, 6, 8, 12, 16, 24, 32, 48, 64, 96, 128, 192, 256]
    
    # Initialize the results dictionary
    results = {}
    
    # Loop over the Gann angles
    for angle in gann_angles:
        # Calculate the number of days in the cycle
        cycle_days = angle * 24
        
        # Calculate the number of cycles in the data
        num_cycles = len(df) // cycle_days
        
        # Initialize the list of cycle HL ranges
        cycle_hl_ranges = []
        
        # Loop over the cycles
        for i in range(num_cycles):
            # Calculate the start and end indexes of the cycle
            start_idx = i * cycle_days
            end_idx = (i + 1) * cycle_days
            
            # Extract the cycle data
            cycle_data = df.iloc[start_idx:end_idx]
            
            # Calculate the cycle HL range
            cycle_hl_range = cycle_data['HL Range'].sum()
            
            # Append the cycle HL range to the list
            cycle_hl_ranges.append(cycle_hl_range)
        
        # Calculate the average cycle HL range
        avg_cycle_hl_range = sum(cycle_hl_ranges) / len(cycle_hl_ranges)
        
        # Calculate the Gann cycle value
        gann_cycle = avg_cycle_hl_range * angle
        
        # Add the Gann cycle value to the results dictionary
        results[angle] = gann_cycle
